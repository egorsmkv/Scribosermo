# Define the network for training.
# Only "name" is a required argument, the others depend on the network you want to use.
# Checkout the network's source code to see which parameters are required to initialize it.
network:
  name: "quartznet"
  blocks: 5
  module_repeat: 5

# All "{LANGUAGE}" occurrences in this file will be replaced with this value automatically.
language: "de"

# Columns "filepath", "duration" and "text" are required, additional columns will be ignored.
data_paths:
  train: "/data_prepared/{LANGUAGE}/voxforge/train_azce.csv"
  eval: "/data_prepared/{LANGUAGE}/voxforge/dev_azce.csv"
  test: "/data_prepared/{LANGUAGE}/voxforge/test_azce.csv"

alphabet_path: "/deepspeech-polyglot/data/alphabet_{LANGUAGE}.json"
scorer_path: "/data_prepared/texts/{LANGUAGE}/kenlm_{LANGUAGE}.scorer"
scorer_alphabet: "/deepspeech-polyglot/data/alphabet_{LANGUAGE}.txt"
audio_sample_rate: 16000

# Depends on your gpu memory. If using multiple gpus, the value will be automatically multiplied.
# In most cases a higher value is better, but if it's too high, you will get an Out-Of-Memory error.
batch_sizes:
  train: 8
  eval: 8
  test: 1

# Sort the datasets. You can use a descending sort to estimate the maximal batch size you can use.
sort_datasets: true
sort_ds_ascending: true

training_epochs: 30

# Early stop training if there was no improvement over the last epochs.
use_early_stopping: true
early_stopping_epochs: 7

# Reduce learning rate by given factor if there was no improvement over the last epochs.
use_lrp_reduction: true
reduce_lr_plateau_epochs: 3
lr_plateau_reduction: 0.1

# Minimum reduction of loss to count as improvement.
# Used for early stopping and reducing learning rate on plateaus.
esrp_min_delta: 0.1

# If there are old checkpoints in the directory, the training will be continued from there
empty_ckpt_dir: true
checkpoint_dir: "/checkpoints/{LANGUAGE}/tmp/"
autosave_every_min: 30

# Copy another checkpoint and continue from there.
# If using this, make sure your config matches the one of the pretrained checkpoint.
continue_pretrained: false
pretrained_checkpoint_dir: "/checkpoints/en/qnet5/"

use_pipeline_cache: true
empty_cache_dir: true
cache_dir: "/tmp/dspol_cache/"

# Print a single greedy prediction while training, disable by setting value to zero.
log_prediction_steps: 25

# Print files with worst predictions. Sort either by "loss" or "greedy_cer". Usable for debugging.
log_worst_test_losses: 3
sort_wtl_key: "greedy_cer"

# Like with the network, checkout the training source code to see which parameters are required.
optimizer:
  name: "novograd"
  learning_rate: 0.0001
  weight_decay: 0.001
  beta1: 0.8
  beta2: 0.5

# Changing things here will break compatibility with the default production bindings.
audio_features:
  use_type: "lfbank"
  mfcc:
    num_features: 26
    window_len: 0.032
    window_step: 0.020
  lfbank:
    num_features: 64
    window_len: 0.020
    window_step: 0.010

augmentations:
  normalize:
    use: true
  resample:
    # Resamples to given sample rate and back again.
    use: false
    temp_sample_rate: 8000
  speed_change:
    # Applies random speed changes, using clipped normal distribution.
    use: false
    mean: 1.0
    stddev: 0.25
    cut_min: 0.5
    cut_max: 2.0
